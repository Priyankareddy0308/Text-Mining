{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial naïve Bayes for Fake Review Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lie</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'Mike\\'s Pizza High Point, NY Service was very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'i really like this buffet restaurant in Marsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'After I went shopping with some of my friend,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'Olive Oil Garden was very disappointing. I ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'The Seven Heaven restaurant was never known f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>'Pastablities is a locally owned restaurant in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>'I like the Pizza at Dominoes for their specia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>'It was a really amazing Japanese restaurant. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>'How do I even pick a best experience at Joe\\'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>'My sister and I ate at this restaurant called...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lie sentiment                                             review\n",
       "0    f         n  'Mike\\'s Pizza High Point, NY Service was very...\n",
       "1    f         n  'i really like this buffet restaurant in Marsh...\n",
       "2    f         n  'After I went shopping with some of my friend,...\n",
       "3    f         n  'Olive Oil Garden was very disappointing. I ex...\n",
       "4    f         n  'The Seven Heaven restaurant was never known f...\n",
       "..  ..       ...                                                ...\n",
       "87   t         p  'Pastablities is a locally owned restaurant in...\n",
       "88   t         p  'I like the Pizza at Dominoes for their specia...\n",
       "89   t         p  'It was a really amazing Japanese restaurant. ...\n",
       "90   t         p  'How do I even pick a best experience at Joe\\'...\n",
       "91   t         p  'My sister and I ate at this restaurant called...\n",
       "\n",
       "[92 rows x 3 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as p\n",
    "file=p.read_csv(\"deception_data_converted_final.tsv\", delimiter='\\t')\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = file['review'].values\n",
    "y_sentiment = file['sentiment'].values\n",
    "y_authenticity = file['lie'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test split for Sentiment anlaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,) (64,) (28,) (28,)\n",
      "'The restaurant looked pretty good, the people around me all ate and talked happily. The environment was comfortable and it calmed me down from the busy life. The food tasted fantastic and the price was suitable'\n",
      "p\n",
      "'After I went shopping with some of my friend, we went to DODO restaurant for dinner. I found worm in one of the dishes .'\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train_st, y_test_st = train_test_split(X, y_sentiment, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train_st.shape, X_test.shape, y_test_st.shape)\n",
    "print(X_train[0])\n",
    "print(y_train_st[0])\n",
    "print(X_test[0])\n",
    "print(y_test_st[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['n' 'p']\n",
      " [31 33]]\n"
     ]
    }
   ],
   "source": [
    "# Check how many training examples in each category\n",
    "# this is important to see whether the data set is balanced or skewed\n",
    "\n",
    "unique, counts = np.unique(y_train_st, return_counts=True)\n",
    "print(np.asarray((unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['n' 'p']\n",
      " [15 13]]\n"
     ]
    }
   ],
   "source": [
    "#Checking on the test data\n",
    "unique, counts = np.unique(y_test_st, return_counts=True)\n",
    "print(np.asarray((unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  unigram term frequency vectorizer\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', min_df=2, binary=False, stop_words = 'english')\n",
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=2, stop_words = 'english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 310)\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "(64, 310)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.30631198 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.32957447 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.27352535 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.12864046 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.17222469 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.26106045 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.32957447 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.27352535 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.32957447 0.28826819\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.12642493 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.32957447 0.         0.         0.         0.         0.\n",
      "  0.         0.32957447 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "X_trainst_vec1 = unigram_count_vectorizer.fit_transform(X_train)\n",
    "X_trainst_vec2 = unigram_tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# check the content of a document vector for unigram vectorizer\n",
    "print(X_trainst_vec1.shape)\n",
    "print(X_trainst_vec1[0].toarray())\n",
    " \n",
    "# check the content of a document vector for tfidf vectorizer\n",
    "print(X_trainst_vec2.shape)\n",
    "print(X_trainst_vec2[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n",
      "[('restaurant', 219), ('looked', 156), ('pretty', 202), ('good', 112), ('people', 193), ('ate', 12), ('talked', 259), ('environment', 80), ('comfortable', 50), ('life', 147)]\n",
      "310\n",
      "[('restaurant', 219), ('looked', 156), ('pretty', 202), ('good', 112), ('people', 193), ('ate', 12), ('talked', 259), ('environment', 80), ('comfortable', 50), ('life', 147)]\n"
     ]
    }
   ],
   "source": [
    "# check the size of the constructed vocabulary\n",
    "print(len(unigram_count_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "print(list(unigram_count_vectorizer.vocabulary_.items())[:10])\n",
    "\n",
    "# check the size of the constructed vocabulary\n",
    "print(len(unigram_tfidf_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "print(list(unigram_tfidf_vectorizer.vocabulary_.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 310)\n",
      "(28, 310)\n"
     ]
    }
   ],
   "source": [
    "X_testst_vec1 = unigram_count_vectorizer.transform(X_test)\n",
    "# print out #examples and #features in the test set\n",
    "print(X_testst_vec1.shape)\n",
    "\n",
    "X_testst_vec2 = unigram_tfidf_vectorizer.transform(X_test)\n",
    "# print out #examples and #features in the test set\n",
    "print(X_testst_vec2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a MNB Classsifier using Count Vectorizer on analysing Sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n' 'p']\n",
      "(2, 310)\n",
      "['n' 'p']\n",
      "(2, 310)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# initialize the MNB model\n",
    "nb_clf1= MultinomialNB()\n",
    "nb_clf2= MultinomialNB()\n",
    "\n",
    "# use the training data to train the MNB model\n",
    "# feature_log_prob_ stores the conditional probs for all categories\n",
    "# if the labels are strings, the index is in alphabetic order\n",
    "# e.g. 'f' comes before 't' in alphabet, so 'f' is in [0] dimension and 't' in [1]\n",
    "\n",
    "nb_clf1.fit(X_trainst_vec1,y_train_st)\n",
    "print(nb_clf1.classes_)\n",
    "print(nb_clf1.feature_log_prob_.shape)\n",
    "\n",
    "nb_clf2.fit(X_trainst_vec2,y_train_st)\n",
    "print(nb_clf2.classes_)\n",
    "print(nb_clf2.feature_log_prob_.shape)\n",
    "#print(nb_clf.classes_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative Feature ranks using Count Vectorizer\n",
      "[(-4.685828089005546, 'asked'), (-4.590517909201221, 'good'), (-4.590517909201221, 'salad'), (-4.503506532211592, 'experience'), (-4.4234638245380555, 'minutes'), (-4.4234638245380555, 'service'), (-4.280362980897381, 'went'), (-4.155199837943376, 'place'), (-3.5872158003374364, 'food'), (-3.5226772791998653, 'restaurant')]\n",
      "Negative Feature ranks using tfidf Vectorizer\n",
      "[(-5.227846168107487, 'asked'), (-5.222577154051034, 'terrible'), (-5.2103084299962354, 'salad'), (-5.187166247253829, 'dishes'), (-5.143151384891839, 'bad'), (-5.133996987257023, 'minutes'), (-5.1030489617666035, 'went'), (-4.938763368455484, 'place'), (-4.8430324276305585, 'food'), (-4.734849461080151, 'restaurant')]\n"
     ]
    }
   ],
   "source": [
    "#sort the conditional probability for category 0 \"negative\"\n",
    "print(\"negative Feature ranks using Count Vectorizer\")\n",
    "feature_ranks = sorted(zip(nb_clf1.feature_log_prob_[0], unigram_count_vectorizer.get_feature_names()))\n",
    "negative_features = feature_ranks[-10:]\n",
    "print(negative_features)\n",
    "\n",
    "print(\"Negative Feature ranks using tfidf Vectorizer\")\n",
    "feature_ranks = sorted(zip(nb_clf2.feature_log_prob_[0], unigram_tfidf_vectorizer.get_feature_names()))\n",
    "negative_features = feature_ranks[-10:]\n",
    "print(negative_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive Feature ranks using Count Vectorizer\n",
      "[(-4.558078578454241, 'fresh'), (-4.558078578454241, 'place'), (-4.558078578454241, 'really'), (-4.558078578454241, 'service'), (-4.462768398649916, 'great'), (-4.375757021660286, 'amazing'), (-4.152613470346077, 'good'), (-3.7251694555191373, 'best'), (-3.682609841100341, 'restaurant'), (-3.564826805443958, 'food')]\n",
      "Positive Feature ranks using tfidf Vectorizer\n",
      "[(-5.219359097661734, 'nice'), (-5.181737794729136, 'really'), (-5.177689513221689, 'friendly'), (-5.141203834989176, 'fresh'), (-5.0652075388691316, 'amazing'), (-4.991196381116305, 'good'), (-4.9816342151921384, 'great'), (-4.842458587375431, 'restaurant'), (-4.7986462614697345, 'food'), (-4.741718621185967, 'best')]\n"
     ]
    }
   ],
   "source": [
    "#sort the conditional probability for category 1 \"Postivie\"\n",
    "print(\"positive Feature ranks using Count Vectorizer\")\n",
    "feature_ranks = sorted(zip(nb_clf1.feature_log_prob_[1], unigram_count_vectorizer.get_feature_names()))\n",
    "positive_features = feature_ranks[-10:]\n",
    "print(positive_features)\n",
    "\n",
    "print(\"Positive Feature ranks using tfidf Vectorizer\")\n",
    "feature_ranks = sorted(zip(nb_clf2.feature_log_prob_[1], unigram_tfidf_vectorizer.get_feature_names()))\n",
    "positive_features = feature_ranks[-10:]\n",
    "print(positive_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n' 'p']\n",
      "(2, 310)\n",
      "[(-2.174835582442741, 'asked'), (-2.069475066784915, 'terrible'), (-2.069475066784915, 'took'), (-1.9516920311285313, 'said'), (-1.8181606385040086, 'came'), (-1.8181606385040086, 'come'), (-1.8181606385040086, 'indian'), (-1.6640099586767496, 'worst'), (-1.4816884018827956, 'bread'), (-1.4816884018827956, 'calling')]\n",
      "[(1.6318269073275786, 'friendly'), (1.7371874229854054, 'atmosphere'), (1.7371874229854054, 'flavors'), (1.7371874229854054, 'fresh'), (1.7371874229854054, 'ice'), (1.7371874229854054, 'ingredients'), (1.7371874229854054, 'noodle'), (1.7371874229854054, 'special'), (1.8769493653605638, 'best'), (2.6126561603393057, 'amazing')]\n"
     ]
    }
   ],
   "source": [
    "# feature analysis for sentiment using Unigram count vectorizer\n",
    "log_ratios = []\n",
    "features = unigram_count_vectorizer.get_feature_names()\n",
    "print(nb_clf1.classes_)\n",
    "print(nb_clf1.feature_log_prob_.shape)\n",
    "log_ratios = nb_clf1.feature_log_prob_[1] - nb_clf1.feature_log_prob_[0]\n",
    "\n",
    "exercise_C_ranks = sorted(zip(log_ratios, features))\n",
    "print(exercise_C_ranks[:10])\n",
    "print(exercise_C_ranks[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n' 'p']\n",
      "(2, 310)\n",
      "[(-0.8392835260607994, 'terrible'), (-0.8340145120043472, 'asked'), (-0.8185685631223674, 'bad'), (-0.7775223169269054, 'took'), (-0.7449907213541067, 'said'), (-0.7212540980234792, 'indian'), (-0.71116743345695, 'come'), (-0.6833202704970631, 'calling'), (-0.65438056224255, 'worst'), (-0.6507018512260512, 'minutes')]\n",
      "[(0.6415435014206254, 'ate'), (0.6632204158005521, 'need'), (0.672675609201967, 'ice'), (0.7088874544298278, 'noodle'), (0.7393270662265934, 'atmosphere'), (0.7617007305608157, 'friendly'), (0.8474594141067371, 'fresh'), (0.8693826879421644, 'great'), (1.0148539818198117, 'amazing'), (1.0810857768119515, 'best')]\n"
     ]
    }
   ],
   "source": [
    "# feature analysis for sentiment using tfidf count vectorizer\n",
    "log_ratios = []\n",
    "features = unigram_tfidf_vectorizer.get_feature_names()\n",
    "print(nb_clf2.classes_)\n",
    "print(nb_clf2.feature_log_prob_.shape)\n",
    "vneg_cond_prob = nb_clf2.feature_log_prob_[1]\n",
    "vpos_cond_prob = nb_clf2.feature_log_prob_[0]\n",
    "\n",
    "log_ratios = nb_clf2.feature_log_prob_[1] - nb_clf2.feature_log_prob_[0]\n",
    "\n",
    "exercise_C_ranks = sorted(zip(log_ratios, features))\n",
    "print(exercise_C_ranks[:10])\n",
    "print(exercise_C_ranks[-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8928571428571429"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf1.score(X_testst_vec1,y_test_st) #For count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf2.score(X_testst_vec2,y_test_st) # for tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  3]\n",
      " [ 0 13]]\n"
     ]
    }
   ],
   "source": [
    "# For count vectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_st1 = nb_clf1.fit(X_trainst_vec1, y_train_st).predict(X_testst_vec1)\n",
    "cm=confusion_matrix(y_test_st, y_pred_st1, labels=['n','p'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  4]\n",
      " [ 0 13]]\n"
     ]
    }
   ],
   "source": [
    "# For tfidf vectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_st2 = nb_clf2.fit(X_trainst_vec2, y_train_st).predict(X_testst_vec2)\n",
    "cm=confusion_matrix(y_test_st, y_pred_st2, labels=['n','p'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.     0.8125]\n",
      "[0.8 1. ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           n       1.00      0.80      0.89        15\n",
      "           p       0.81      1.00      0.90        13\n",
      "\n",
      "    accuracy                           0.89        28\n",
      "   macro avg       0.91      0.90      0.89        28\n",
      "weighted avg       0.91      0.89      0.89        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report for count vectorizer\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print(precision_score(y_test_st, y_pred_st1, average=None))\n",
    "print(recall_score(y_test_st, y_pred_st1, average=None))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['n','p']\n",
    "print(classification_report(y_test_st, y_pred_st1, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.76470588]\n",
      "[0.73333333 1.        ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           n       1.00      0.73      0.85        15\n",
      "           p       0.76      1.00      0.87        13\n",
      "\n",
      "    accuracy                           0.86        28\n",
      "   macro avg       0.88      0.87      0.86        28\n",
      "weighted avg       0.89      0.86      0.86        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report for tfidf vectorizer\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print(precision_score(y_test_st, y_pred_st2, average=None))\n",
    "print(recall_score(y_test_st, y_pred_st2, average=None))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['n','p']\n",
    "print(classification_report(y_test_st, y_pred_st2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78085003 0.21914997]\n",
      "n\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "## find the calculated posterior probability for count vectorizer\n",
    "posterior_probs1 = nb_clf1.predict_proba(X_testst_vec1)\n",
    "\n",
    "## find the posterior probabilities for the first test example\n",
    "print(posterior_probs1[0])\n",
    "\n",
    "# find the category prediction for the first test example\n",
    "y_pred1 = nb_clf1.predict(X_testst_vec1)\n",
    "print(y_pred1[0])\n",
    "\n",
    "# check the actual label for the first test example\n",
    "print(y_test_st[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55908436 0.44091564]\n",
      "n\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "## find the calculated posterior probability for count vectorizer\n",
    "posterior_probs2 = nb_clf2.predict_proba(X_testst_vec2)\n",
    "\n",
    "## find the posterior probabilities for the first test example\n",
    "print(posterior_probs2[0])\n",
    "\n",
    "# find the category prediction for the first test example\n",
    "y_pred2 = nb_clf2.predict(X_testst_vec2)\n",
    "print(y_pred2[0])\n",
    "\n",
    "# check the actual label for the first test example\n",
    "print(y_test_st[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This place used to be great. I can\\'t believe it\\'s current state. Instead of the cool, dimly-lit lounge that I was used to, I was in a cheap, smelly bar. The music has no soul, the bartender is mean. This place no longer exudes a welcoming spirit. The crowd is awkward and old. I want my old hangout back!!'\n",
      "'Carlo\\'s Plate Shack was the worst dining experience of my life. Although my Southern Comfort Plate sounded to die for, the staff was extremely unhelpful at every turn. We started off with drinks, I had a sick Loganberry milkshake, and my friends had fresh brewed, but bland, iced tea (the ice likely melted and diluted). Eventually our server returned a half hour later to take our orders. I had the aforementioned Southern Comfort Plate, while my friends ordered the Buffalo Chicken Plate and the Hawaiian Plate Lunch. The Southern Comfort Plate came out first, a good 15 minutes before the others, and was extremely greasy. The other 2 ended up being nearly room temperature when they came out. Our server failed to return again to check on us until she brought our check rather abruptly. We want to give this place a chance, but it\\'s rather difficult to subject ourselves to such brutal service and pay money.'\n",
      "'This diner was not at all up to par. I\\'ve been to many diners, and get eggs benedict sometimes. There was nacho cheese on my eggs, and a plateful of watery runny eggs. And it smelled like smoke. And there was no heat, in the dead of winter. Their prices are not ANYWHERE near what is reasonable. Cool mom & pop place, but terrible food, smell, and prices.'\n",
      "errors: 3\n"
     ]
    }
   ],
   "source": [
    "#Negative review predicted as positive for Count vectorizer\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test_st)):\n",
    "    if(y_test_st[i]=='n' and y_pred1[i]=='p'):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This place used to be great. I can\\'t believe it\\'s current state. Instead of the cool, dimly-lit lounge that I was used to, I was in a cheap, smelly bar. The music has no soul, the bartender is mean. This place no longer exudes a welcoming spirit. The crowd is awkward and old. I want my old hangout back!!'\n",
      "'Carlo\\'s Plate Shack was the worst dining experience of my life. Although my Southern Comfort Plate sounded to die for, the staff was extremely unhelpful at every turn. We started off with drinks, I had a sick Loganberry milkshake, and my friends had fresh brewed, but bland, iced tea (the ice likely melted and diluted). Eventually our server returned a half hour later to take our orders. I had the aforementioned Southern Comfort Plate, while my friends ordered the Buffalo Chicken Plate and the Hawaiian Plate Lunch. The Southern Comfort Plate came out first, a good 15 minutes before the others, and was extremely greasy. The other 2 ended up being nearly room temperature when they came out. Our server failed to return again to check on us until she brought our check rather abruptly. We want to give this place a chance, but it\\'s rather difficult to subject ourselves to such brutal service and pay money.'\n",
      "'This diner was not at all up to par. I\\'ve been to many diners, and get eggs benedict sometimes. There was nacho cheese on my eggs, and a plateful of watery runny eggs. And it smelled like smoke. And there was no heat, in the dead of winter. Their prices are not ANYWHERE near what is reasonable. Cool mom & pop place, but terrible food, smell, and prices.'\n",
      "'After reading the reviews as well listening to all my friends rave about Dinosaur Bar-B-Que, I decided to try it out! This is definitely the kind of place for you if you are into crowded, noisy and people-walking-all-over type of person. Unfortunately I am not that type of person. I found the place dirty, noisy, too crowded, and the smell of the people over powered the smell of the food. There were also loads of beer floating around and as the minutes flowed by, people just became rowdier. Although the food was good, I would not rate it as \\'you have to try it before you die\\' kinda food. Sorry Dinosaur Bar-B-Que, you are just not my kinda dig!'\n",
      "errors: 4\n"
     ]
    }
   ],
   "source": [
    "#Negative review predicted as positive for tfidf vectorizer\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test_st)):\n",
    "    if(y_test_st[i]=='n' and y_pred2[i]=='p'):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors: 0\n"
     ]
    }
   ],
   "source": [
    "#Positive review predicted as negative for count vectorizer\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test_st)):\n",
    "    if(y_test_st[i]=='p' and y_pred1[i]=='n'):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors: 0\n"
     ]
    }
   ],
   "source": [
    "#Positive review predicted as negative for TFIDF vectorizer\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test_st)):\n",
    "    if(y_test_st[i]=='p' and y_pred2[i]=='n'):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation on Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8251461988304094\n"
     ]
    }
   ],
   "source": [
    "##MNB with Bool\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=True)),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y_sentiment, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8801169590643274\n"
     ]
    }
   ],
   "source": [
    "##MNB TFIDF\n",
    "mNB_tfidf_pipe = Pipeline([('nb_tf',TfidfVectorizer(encoding='latin-1',use_idf=True,binary=False)),('nb',MultinomialNB())])\n",
    "scores = cross_val_score(mNB_tfidf_pipe,X,y_sentiment,cv=5)\n",
    "print(sum(scores)/len(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test split for authenticity anlaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,) (64,) (28,) (28,)\n",
      "'The restaurant looked pretty good, the people around me all ate and talked happily. The environment was comfortable and it calmed me down from the busy life. The food tasted fantastic and the price was suitable'\n",
      "t\n",
      "'After I went shopping with some of my friend, we went to DODO restaurant for dinner. I found worm in one of the dishes .'\n",
      "f\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train_at, y_test_at = train_test_split(X, y_authenticity, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train_at.shape, X_test.shape, y_test_at.shape)\n",
    "print(X_train[0])\n",
    "print(y_train_at[0])\n",
    "print(X_test[0])\n",
    "print(y_test_at[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['f' 't']\n",
      " [33 31]]\n"
     ]
    }
   ],
   "source": [
    "# Check how many training examples in each category\n",
    "# this is important to see whether the data set is balanced or skewed\n",
    "\n",
    "unique, counts = np.unique(y_train_at, return_counts=True)\n",
    "print(np.asarray((unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  unigram term frequency vectorizer\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', min_df=2, binary=False, stop_words = 'english')\n",
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 310)\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "(64, 310)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.30631198 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.32957447 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.27352535 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.12864046 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.17222469 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.26106045 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.32957447 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.27352535 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.32957447 0.28826819\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.12642493 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.32957447 0.         0.         0.         0.         0.\n",
      "  0.         0.32957447 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "X_trainat_vec1 = unigram_count_vectorizer.fit_transform(X_train)\n",
    "X_trainat_vec2 = unigram_tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# check the content of a document vector for unigram vectorizer\n",
    "print(X_trainat_vec1.shape)\n",
    "print(X_trainat_vec1[0].toarray())\n",
    " \n",
    "# check the content of a document vector for tfidf vectorizer\n",
    "print(X_trainat_vec2.shape)\n",
    "print(X_trainat_vec2[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n",
      "[('restaurant', 219), ('looked', 156), ('pretty', 202), ('good', 112), ('people', 193), ('ate', 12), ('talked', 259), ('environment', 80), ('comfortable', 50), ('life', 147)]\n",
      "310\n",
      "[('restaurant', 219), ('looked', 156), ('pretty', 202), ('good', 112), ('people', 193), ('ate', 12), ('talked', 259), ('environment', 80), ('comfortable', 50), ('life', 147)]\n"
     ]
    }
   ],
   "source": [
    "# check the size of the constructed vocabulary\n",
    "print(len(unigram_count_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "print(list(unigram_count_vectorizer.vocabulary_.items())[:10])\n",
    "\n",
    "# check the size of the constructed vocabulary\n",
    "print(len(unigram_tfidf_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "print(list(unigram_tfidf_vectorizer.vocabulary_.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 310)\n",
      "(28, 310)\n"
     ]
    }
   ],
   "source": [
    "X_testat_vec1 = unigram_count_vectorizer.transform(X_test)\n",
    "# print out #examples and #features in the test set\n",
    "print(X_testat_vec1.shape)\n",
    "\n",
    "X_testat_vec2 = unigram_tfidf_vectorizer.transform(X_test)\n",
    "# print out #examples and #features in the test set\n",
    "print(X_testat_vec2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training a MNB Classsifier using Count Vectorizer on analysing authenticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f' 't']\n",
      "(2, 310)\n",
      "['f' 't']\n",
      "(2, 310)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# initialize the MNB model\n",
    "nb_clf3= MultinomialNB()\n",
    "nb_clf4= MultinomialNB()\n",
    "\n",
    "# use the training data to train the MNB model\n",
    "# feature_log_prob_ stores the conditional probs for all categories\n",
    "# if the labels are strings, the index is in alphabetic order\n",
    "# e.g. 'f' comes before 't' in alphabet, so 'f' is in [0] dimension and 't' in [1]\n",
    "\n",
    "nb_clf3.fit(X_trainat_vec1,y_train_at)\n",
    "print(nb_clf3.classes_)\n",
    "print(nb_clf3.feature_log_prob_.shape)\n",
    "\n",
    "nb_clf4.fit(X_trainat_vec2,y_train_at)\n",
    "print(nb_clf4.classes_)\n",
    "print(nb_clf4.feature_log_prob_.shape)\n",
    "#print(nb_clf.classes_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake Feature ranks using Count Vectorizer\n",
      "[(-4.599700710183556, 'good'), (-4.599700710183556, 'like'), (-4.512689333193926, 'minutes'), (-4.512689333193926, 'went'), (-4.358538653366669, 'experience'), (-4.289545781879717, 'service'), (-4.225007260742146, 'best'), (-4.225007260742146, 'place'), (-3.819542152633981, 'restaurant'), (-3.596398601319771, 'food')]\n",
      "fake Feature ranks using tfidf Vectorizer\n",
      "[(-5.222621534700683, 'menu'), (-5.204141206095463, 'minutes'), (-5.194766449563898, 'want'), (-5.191439930725854, 'like'), (-5.169589079803627, 'experience'), (-5.091523346372281, 'service'), (-5.036297924807053, 'place'), (-4.9902384138165345, 'best'), (-4.931710867417781, 'restaurant'), (-4.8395399172187705, 'food')]\n"
     ]
    }
   ],
   "source": [
    "print(\"fake Feature ranks using Count Vectorizer\")\n",
    "feature_ranks = sorted(zip(nb_clf3.feature_log_prob_[0], unigram_count_vectorizer.get_feature_names()))\n",
    "negative_features = feature_ranks[-10:]\n",
    "print(negative_features)\n",
    "\n",
    "print(\"fake Feature ranks using tfidf Vectorizer\")\n",
    "feature_ranks = sorted(zip(nb_clf4.feature_log_prob_[0], unigram_tfidf_vectorizer.get_feature_names()))\n",
    "negative_features = feature_ranks[-10:]\n",
    "print(negative_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true Feature ranks using Count Vectorizer\n",
      "[(-4.65290158880928, 'really'), (-4.65290158880928, 'salad'), (-4.65290158880928, 'time'), (-4.5475410731514545, 'ordered'), (-4.5475410731514545, 'went'), (-4.45223089334713, 'best'), (-4.45223089334713, 'place'), (-4.14207596504329, 'good'), (-3.5542893001411713, 'food'), (-3.384390263345774, 'restaurant')]\n",
      "true Feature ranks using tfidf Vectorizer\n",
      "[(-5.3097769800584675, 'went'), (-5.279450072677803, 'bad'), (-5.2643763095714355, 'dishes'), (-5.242117858118911, 'really'), (-5.22604674402891, 'salad'), (-5.209498165662844, 'ordered'), (-5.123975695678814, 'place'), (-5.008451736461623, 'good'), (-4.801344505308265, 'food'), (-4.653277927142044, 'restaurant')]\n"
     ]
    }
   ],
   "source": [
    "print(\"true Feature ranks using Count Vectorizer\")\n",
    "feature_ranks = sorted(zip(nb_clf3.feature_log_prob_[1], unigram_count_vectorizer.get_feature_names()))\n",
    "positive_features = feature_ranks[-10:]\n",
    "print(positive_features)\n",
    "\n",
    "print(\"true Feature ranks using tfidf Vectorizer\")\n",
    "feature_ranks = sorted(zip(nb_clf4.feature_log_prob_[1], unigram_tfidf_vectorizer.get_feature_names()))\n",
    "positive_features = feature_ranks[-10:]\n",
    "print(positive_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-2.049754760499793, 'plate'), (-1.9319717248434092, 'want'), (-1.6442896523916293, 'cold'), (-1.6442896523916293, 'delicious'), (-1.4619680955976744, '15'), (-1.4619680955976744, 'bread'), (-1.4619680955976744, 'cooked'), (-1.4619680955976744, 'free'), (-1.4619680955976744, 'ice'), (-1.4619680955976744, 'outstanding')]\n",
      "[(1.533764177956317, 'did'), (1.533764177956317, 'ignored'), (1.533764177956317, 'makes'), (1.533764177956317, 'pretty'), (1.533764177956317, 'shrimp'), (1.533764177956317, 'thing'), (1.7569077292705266, 'calling'), (1.7569077292705266, 'tables'), (1.9392292860644806, 'worst'), (2.2269113585162623, 'glass')]\n"
     ]
    }
   ],
   "source": [
    "# feature analysis for sentiment using Unigram count vectorizer\n",
    "log_ratios = []\n",
    "features = unigram_count_vectorizer.get_feature_names()\n",
    "vneg_cond_prob = nb_clf3.feature_log_prob_[1]\n",
    "vpos_cond_prob = nb_clf3.feature_log_prob_[0]\n",
    "\n",
    "log_ratios = nb_clf3.feature_log_prob_[1] - nb_clf3.feature_log_prob_[0]\n",
    "\n",
    "exercise_C_ranks = sorted(zip(log_ratios, features))\n",
    "print(exercise_C_ranks[:10])\n",
    "print(exercise_C_ranks[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.8478165835813405, 'want'), (-0.6619293787971818, 'delicious'), (-0.6190970810570091, 'cold'), (-0.6007795313464701, 'steak'), (-0.5984212150128023, 'ice'), (-0.5789412885558667, 'plate'), (-0.5445522419926903, 'free'), (-0.5357420718877588, 'cooked'), (-0.5281734588758047, 'bring'), (-0.5079079137785811, 'beer')]\n",
      "[(0.5676775438136312, 'suitable'), (0.5710307503215892, 'smile'), (0.5878328278079206, 'big'), (0.6079274574779987, 'shrimp'), (0.6169490822281132, 'environment'), (0.671896846583147, 'did'), (0.7286349564317156, 'worst'), (0.7312451103525666, 'glass'), (0.7369803193402422, 'tables'), (0.7575746646862287, 'calling')]\n"
     ]
    }
   ],
   "source": [
    "# feature analysis for sentiment using tfidf count vectorizer\n",
    "log_ratios = []\n",
    "features = unigram_count_vectorizer.get_feature_names()\n",
    "vneg_cond_prob = nb_clf4.feature_log_prob_[1]\n",
    "vpos_cond_prob = nb_clf4.feature_log_prob_[0]\n",
    "\n",
    "log_ratios = nb_clf4.feature_log_prob_[1] - nb_clf4.feature_log_prob_[0]\n",
    "\n",
    "exercise_C_ranks = sorted(zip(log_ratios, features))\n",
    "print(exercise_C_ranks[:10])\n",
    "print(exercise_C_ranks[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5357142857142857"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf3.score(X_testat_vec1,y_test_at) #For count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5357142857142857"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf4.score(X_testat_vec2,y_test_at) # for tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 4]\n",
      " [9 6]]\n"
     ]
    }
   ],
   "source": [
    "# For count vectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_at1 = nb_clf3.fit(X_trainat_vec1, y_train_at).predict(X_testat_vec1)\n",
    "cm=confusion_matrix(y_test_at, y_pred_at1, labels=['f','t'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  3]\n",
      " [10  5]]\n"
     ]
    }
   ],
   "source": [
    "# For tf_idf vectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_at2 = nb_clf4.fit(X_trainat_vec2, y_train_at).predict(X_testat_vec2)\n",
    "cm=confusion_matrix(y_test_at, y_pred_at2, labels=['f','t'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.6]\n",
      "[0.69230769 0.4       ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           f       0.50      0.69      0.58        13\n",
      "           t       0.60      0.40      0.48        15\n",
      "\n",
      "    accuracy                           0.54        28\n",
      "   macro avg       0.55      0.55      0.53        28\n",
      "weighted avg       0.55      0.54      0.53        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report for count vectorizer\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print(precision_score(y_test_at, y_pred_at1, average=None))\n",
    "print(recall_score(y_test_at, y_pred_at1, average=None))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['f','t']\n",
    "print(classification_report(y_test_at, y_pred_at1, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5   0.625]\n",
      "[0.76923077 0.33333333]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           f       0.50      0.77      0.61        13\n",
      "           t       0.62      0.33      0.43        15\n",
      "\n",
      "    accuracy                           0.54        28\n",
      "   macro avg       0.56      0.55      0.52        28\n",
      "weighted avg       0.57      0.54      0.51        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report for tfidf vectorizer\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print(precision_score(y_test_at, y_pred_at2, average=None))\n",
    "print(recall_score(y_test_at, y_pred_at2, average=None))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['f','t']\n",
    "print(classification_report(y_test_at, y_pred_at2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65496973 0.34503027]\n",
      "n\n",
      "f\n"
     ]
    }
   ],
   "source": [
    "## find the calculated posterior probability for count vectorizer\n",
    "posterior_probs3 = nb_clf3.predict_proba(X_testat_vec1)\n",
    "\n",
    "## find the posterior probabilities for the first test example\n",
    "print(posterior_probs3[0])\n",
    "\n",
    "# find the category prediction for the first test example\n",
    "y_pred3 = nb_clf3.predict(X_testat_vec1)\n",
    "print(y_pred1[0])\n",
    "\n",
    "# check the actual label for the first test example\n",
    "print(y_test_at[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51726141 0.48273859]\n",
      "f\n",
      "f\n"
     ]
    }
   ],
   "source": [
    "## find the calculated posterior probability for tfidf vectorizer\n",
    "posterior_probs4 = nb_clf4.predict_proba(X_testat_vec2)\n",
    "\n",
    "## find the posterior probabilities for the first test example\n",
    "print(posterior_probs4[0])\n",
    "\n",
    "# find the category prediction for the first test example\n",
    "y_pred4 = nb_clf4.predict(X_testat_vec2)\n",
    "print(y_pred4[0])\n",
    "\n",
    "# check the actual label for the first test example\n",
    "print(y_test_at[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I ate at this restaurant called Banana Leaf. As I entered the restaurant I really liked the ambiance. I ordered noodle soup and fried rice with spicy black bean curry. The service was pretty fast and the food tasted amazing. There was a lot flavor in the food which I truly enjoyed. Two thumbs up for Banana Leaf and I would totally recommend this restaurant.'\n",
      "'OMG. This restaurant is horrible. The receptionist did not greet us, we just stood there and waited for five minutes. The food came late and served not warm. Me and my pet ordered a bowl of salad and a cheese pizza. The salad was not fresh, the crust of a pizza was so hard like plastics. My dog didn\\'t even eat that pizza. I hate this place!!!!!!!!!!'\n",
      "'This restaurant ROCKS! I mean the food is great and people are great. Everything is great great just great!!! I love it. I like it. '\n",
      "'I went to ABC restaurant two days ago and I hated the food and the service. We were kept waiting for over an hour just to get seated and once we ordered, our food came out cold. I ordered the pasta and it was terrible - completely bland and very unappatizing. I definitely would not recommend going there, especially if you\\'re in a hurry!'\n",
      "errors: 4\n"
     ]
    }
   ],
   "source": [
    "#fake review predicted as true review for Count vectorizer\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test_at)):\n",
    "    if(y_test_at[i]=='f' and y_pred3[i]=='t'):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I ate at this restaurant called Banana Leaf. As I entered the restaurant I really liked the ambiance. I ordered noodle soup and fried rice with spicy black bean curry. The service was pretty fast and the food tasted amazing. There was a lot flavor in the food which I truly enjoyed. Two thumbs up for Banana Leaf and I would totally recommend this restaurant.'\n",
      "'OMG. This restaurant is horrible. The receptionist did not greet us, we just stood there and waited for five minutes. The food came late and served not warm. Me and my pet ordered a bowl of salad and a cheese pizza. The salad was not fresh, the crust of a pizza was so hard like plastics. My dog didn\\'t even eat that pizza. I hate this place!!!!!!!!!!'\n",
      "'I went to ABC restaurant two days ago and I hated the food and the service. We were kept waiting for over an hour just to get seated and once we ordered, our food came out cold. I ordered the pasta and it was terrible - completely bland and very unappatizing. I definitely would not recommend going there, especially if you\\'re in a hurry!'\n",
      "errors: 3\n"
     ]
    }
   ],
   "source": [
    "# True review predicted as fake review for count vectorizer\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test_at)):\n",
    "    if(y_test_at[i]=='f' and y_pred4[i]=='t'):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I ate at this restaurant called Banana Leaf. As I entered the restaurant I really liked the ambiance. I ordered noodle soup and fried rice with spicy black bean curry. The service was pretty fast and the food tasted amazing. There was a lot flavor in the food which I truly enjoyed. Two thumbs up for Banana Leaf and I would totally recommend this restaurant.'\n",
      "'OMG. This restaurant is horrible. The receptionist did not greet us, we just stood there and waited for five minutes. The food came late and served not warm. Me and my pet ordered a bowl of salad and a cheese pizza. The salad was not fresh, the crust of a pizza was so hard like plastics. My dog didn\\'t even eat that pizza. I hate this place!!!!!!!!!!'\n",
      "'I went to ABC restaurant two days ago and I hated the food and the service. We were kept waiting for over an hour just to get seated and once we ordered, our food came out cold. I ordered the pasta and it was terrible - completely bland and very unappatizing. I definitely would not recommend going there, especially if you\\'re in a hurry!'\n",
      "errors: 3\n"
     ]
    }
   ],
   "source": [
    "#fake review predicted as true review for tfidf vectorizer\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test_at)):\n",
    "    if(y_test_at[i]=='f' and y_pred4[i]=='t'):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I ate at this restaurant called Banana Leaf. As I entered the restaurant I really liked the ambiance. I ordered noodle soup and fried rice with spicy black bean curry. The service was pretty fast and the food tasted amazing. There was a lot flavor in the food which I truly enjoyed. Two thumbs up for Banana Leaf and I would totally recommend this restaurant.'\n",
      "'OMG. This restaurant is horrible. The receptionist did not greet us, we just stood there and waited for five minutes. The food came late and served not warm. Me and my pet ordered a bowl of salad and a cheese pizza. The salad was not fresh, the crust of a pizza was so hard like plastics. My dog didn\\'t even eat that pizza. I hate this place!!!!!!!!!!'\n",
      "'I went to ABC restaurant two days ago and I hated the food and the service. We were kept waiting for over an hour just to get seated and once we ordered, our food came out cold. I ordered the pasta and it was terrible - completely bland and very unappatizing. I definitely would not recommend going there, especially if you\\'re in a hurry!'\n",
      "errors: 3\n"
     ]
    }
   ],
   "source": [
    "#fake review predicted as true review for tfidf vectorizer\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test_at)):\n",
    "    if(y_test_at[i]=='f' and y_pred4[i]=='t'):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.587719298245614\n"
     ]
    }
   ],
   "source": [
    "##MNB with Bool\n",
    "nb_clf_pipe3 = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=True)),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe3, X, y_authenticity, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.587719298245614\n"
     ]
    }
   ],
   "source": [
    "##MNB TFIDF\n",
    "mNB_tfidf_pipe4 = Pipeline([('nb_tf',TfidfVectorizer(encoding='latin-1',use_idf=True,binary=False)),('nb',MultinomialNB())])\n",
    "scores = cross_val_score(mNB_tfidf_pipe4,X,y_authenticity,cv=5)\n",
    "print(sum(scores)/len(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
